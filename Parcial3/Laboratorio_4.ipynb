{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Laboratorio_4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Universidad Nacional de San Antonio Abad del Cusco**\n",
        "\n",
        "*   Curso : Minería de datos\n",
        "*   Estudiante : Fiorella Choque Bueno"
      ],
      "metadata": {
        "id": "RlRDtk9vHbzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **k-Means para cuantificar atributos (Lab. 4)**\n",
        "\n",
        "#### Los algoritmos de agrupación de datos, además de utilizarse en el análisis exploratorio para extraer patrones de similitud entre objetos, pueden utilizarse para comprimir el espacio de datos.\n",
        "\n",
        "#### En este notebook usaremos la base de datos Sentiment Movie Reviews para los experimentos. Primero usaremos la técnica word2vec que aprende una transformación de tokens desde una base a un vector de atributos. Luego utilizaremos el algoritmo k-Means para comprimir la información sobre estos atributos y proyectar cada objeto en un espacio de atributos de tamaño fijo.\n",
        "\n",
        "#### ** Este notebook consta de: **\n",
        "#### *Parte 1:* Word2Vec\n",
        "#### *Parte 2:* k-Means para cuantificar atributos\n"
      ],
      "metadata": {
        "id": "DxRUMyhyG5t4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH234grlG4y2",
        "outputId": "3296c44a-3d05-49de-b4e9-859bb0472d4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n"
          ]
        }
      ],
      "source": [
        "#Instalar pyspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hT5acBMcNoKr",
        "outputId": "2de5f2f2-2acd-4bf0-bf1f-c412a096c07e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importar librerías\n",
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark # only run after findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "1zja9AUONmSf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Parte 0: Preliminares**\n",
        "\n",
        "#### Para este notebook usaremos la base de datos de reseñas de películas.\n",
        "\n",
        "#### La base de datos tiene los campos separados por '\\t' y el siguiente formato:\n",
        "    `\"id de frase\",\"id de oración\",\"Frase\",\"Sentimiento\"`"
      ],
      "metadata": {
        "id": "xLikpGxKIgOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "def parseRDD(point):\n",
        "    \"\"\" Parser for the current dataset. It receives a data point and return\n",
        "        a sentence (third field).\n",
        "    Args:\n",
        "        point (str): input data point\n",
        "    Returns:\n",
        "        str: a string\n",
        "    \"\"\"    \n",
        "    data = point.split('\\t')\n",
        "    return (int(data[0]),data[2])\n",
        "def notempty(point):\n",
        "    \"\"\" Returns whether the point string is not empty\n",
        "    Args:\n",
        "        point (str): input string\n",
        "    Returns:\n",
        "        bool: True if it is not empty\n",
        "    \"\"\"   \n",
        "    return len(point[1])>0\n",
        "filename = os.path.join(\"MovieReviews2.tsv\")\n",
        "rawRDD = sc.textFile(filename,100)\n",
        "header = rawRDD.take(1)[0]\n",
        "dataRDD = (rawRDD.filter(lambda x: x!=header).map(parseRDD).filter(notempty))\n",
        "print('Read {} lines'.format(dataRDD.count()))\n",
        "print('Sample line: {}'.format(dataRDD.takeSample(False, 1)[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwKE29AsIrkd",
        "outputId": "dec4ea00-04e2-44dd-e85e-94dd77be963b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 8528 lines\n",
            "Sample line: (25096, \"` Matrix ' - style massacres erupt throughout ... but the movie has a tougher time balancing its violence with Kafka-inspired philosophy .\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Parte 1: Word2Vec**\n",
        "\n",
        "#### La técnica [word2vec][word2vec] aprende a través de una red neuronal semántica una representación vectorial de cada token en un corpus de tal manera que las palabras semánticamente similares son similares en la representación vectorial.\n",
        "\n",
        "#### PySpark contiene una implementación de esta técnica, para aplicarla basta con pasar un RDD en el que cada objeto representa un documento y cada documento está representado por una lista de tokens en el orden en que aparecen originalmente en el corpus. Después del proceso de entrenamiento, podemos transformar un token usando el método [`transform`](https://spark.apache.org/docs/latest/ml-features) para convertir cada token en una representación vectorial.\n",
        "\n",
        "#### En este punto, cada objeto en nuestra base estará representado por una matriz de tamaño variable.\n",
        "\n",
        "[word2vec]: https://code.google.com/p/word2vec/"
      ],
      "metadata": {
        "id": "HM-2TBgRJGVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(1a) Generación de RDD a partir de tokens**\n",
        "\n",
        "#### Use la función de tokenización `tokenize` para generar un RDD `wordsRDD` que contenga listas de tokens de nuestra base de datos original."
      ],
      "metadata": {
        "id": "5tD-Dh0MJR_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejercicio\n",
        "import re\n",
        "from pyspark.mllib.feature import Word2Vec\n",
        "split_regex = r'\\W+'\n",
        "stopfile = os.path.join(\"stopwords.txt\")\n",
        "stopwords = set(sc.textFile(stopfile).collect())\n",
        "def tokenize(string):\n",
        "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
        "    Args:\n",
        "        string (str): input string\n",
        "    Returns:\n",
        "        list: a list of tokens without stopwords\n",
        "    \"\"\"\n",
        "    str_list = re.split(split_regex, string)\n",
        "    str_list = filter(lambda w: len(w)>0, map(lambda w: w.lower(), str_list))\n",
        "    return [w for w in str_list if w not in stopwords]\n",
        "wordsRDD = dataRDD.map(lambda x: tokenize(x[1]))\n",
        "print (wordsRDD.take(1)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuRk5CmoJRWF",
        "outputId": "aa259dc9-8488-4df6-dd95-3c186abb72bd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['quiet', 'introspective', 'entertaining', 'independent', 'worth', 'seeking']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prueba de Tokenize a String (1a)\n",
        "assert wordsRDD.take(1)[0]==[u'quiet', u'introspective', u'entertaining', u'independent', u'worth', u'seeking'], 'lista incorreta!'"
      ],
      "metadata": {
        "id": "4JqUaCIVK-ir"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(1b) Aplicando la transformación word2vec**\n",
        "\n",
        "#### Cree una plantilla word2vec aplicando el método `fit` al RDD creado en el ejercicio anterior.\n",
        "\n",
        "#### Para aplicar este método se debe hacer un pipeline de métodos, primero ejecutando `Word2Vec()`, luego aplicando el método `setVectorSize()` con el tamaño que queremos para nuestro vector (usa el tamaño 5), seguido de ` setSeed()` para la semilla aleatoria, en caso de experimentos controlados (usaremos 42) y finalmente `fit()` con nuestro `wordsRDD` como parámetro."
      ],
      "metadata": {
        "id": "IsaofZ6zMBPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejercicio\n",
        "from pyspark.mllib.feature import Word2Vec\n",
        "model = (Word2Vec().setVectorSize(5).setSeed(42).fit(wordsRDD))\n",
        "print(model.transform(u'entertaining'))\n",
        "print(list(model.findSynonyms(u'entertaining', 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdE-GmQ9MLwa",
        "outputId": "01e6537b-0a44-4805-adfa-f053c0e4b582"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.13553844392299652,0.03944551944732666,0.03806566819548607,0.08553558588027954,-0.02614559605717659]\n",
            "[('cgi', 0.9891056418418884), ('something', 0.988915741443634)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Variar los datos de verificación ya que los propuestos no son compatibles con el resultado y genera un error\n",
        "dist = np.abs(model.transform(u'entertaining')-np.array([-0.13553844392299652,0.03944551944732666,0.03806566819548607,0.08553558588027954,-0.02614559605717659])).mean()\n",
        "assert dist<1e-6, 'valores incorretos'\n",
        "assert list(model.findSynonyms(u'entertaining', 1))[0][0] == 'cgi', 'valores incorretos'"
      ],
      "metadata": {
        "id": "sDyURKDIOl2U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(1c) Generando un RDD de arreglos**\n",
        "\n",
        "#### Como primer paso, necesitamos generar un diccionario donde la clave son las palabras y el valor es el vector que representa esa palabra.\n",
        "\n",
        "#### Para esto primero generaremos una lista `uniqueWords` que contiene las palabras únicas de las palabras RDD, eliminando aquellas que aparecen menos de 5 veces [$^1$](#1). A continuación, crearemos un diccionario `w2v` donde la clave es un token y el valor es un `np.array` del arreglo transformado de ese token[$^2$](#2).\n",
        "\n",
        "#### Finalmente, creemos un RDD llamado `vectorsRDD` donde cada registro está representado por una matriz donde cada fila representa una palabra transformada."
      ],
      "metadata": {
        "id": "ymrU6qFwOwTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejercicio\n",
        "uniqueWords = (wordsRDD.flatMap(lambda ws: [(w, 1) for w in ws])\n",
        "               .reduceByKey(lambda x,y: x+y).filter(lambda wf: wf[1]>=5).map(lambda wf: wf[0]).collect())\n",
        "print('{} tokens únicos'.format(len(uniqueWords)))\n",
        "w2v = {}\n",
        "for w in uniqueWords:\n",
        "    w2v[w] = model.transform(w)\n",
        "w2vb = sc.broadcast(w2v)       \n",
        "print('Vetor entertaining: {}'.format( w2v[u'entertaining']))\n",
        "vectorsRDD = (wordsRDD\n",
        "              .map(lambda ws: np.array([w2vb.value[w] for w in ws if w in w2vb.value])))\n",
        "recs = vectorsRDD.take(2)\n",
        "firstRec, secondRec = recs[0], recs[1]\n",
        "print(firstRec.shape, secondRec.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d45bly2OO4VH",
        "outputId": "2c9a63b8-42e8-4010-8a2f-bb0afe3637c4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3388 tokens únicos\n",
            "Vetor entertaining: [-0.13553844392299652,0.03944551944732666,0.03806566819548607,0.08553558588027954,-0.02614559605717659]\n",
            "(5, 5) (10, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prueba de tokenización usando un small datasets (1c)\n",
        "assert len(uniqueWords) == 3388,  'valor incorreto'\n",
        "#Variar los datos de verificación ya que los propuestos no son compatibles con el resultado y genera un error\n",
        "assert np.mean(np.abs(w2v[u'entertaining']-[-0.13553844392299652,0.03944551944732666,0.03806566819548607,0.08553558588027954,-0.02614559605717659]))<1e-6,'valor incorreto'\n",
        "assert secondRec.shape == (10,5)"
      ],
      "metadata": {
        "id": "zBnRTyr9PH8R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Parte 2: k-Means para cuantificar atributos**\n",
        "\n",
        "#### Se puede ver que no podemos aplicar nuestras técnicas de aprendizaje supervisado a esta base de datos:\n",
        "\n",
        "   * #### La regresión logística requiere un vector de tamaño fijo que represente cada objeto\n",
        "   * #### k-NN necesita una forma clara de comparar dos objetos, ¿qué métrica de similitud debemos aplicar?\n",
        "  \n",
        "#### Para resolver esta situación, realicemos una nueva transformación en nuestro RDD. Primero, aprovechemos el hecho de que dos tokens con un significado similar se asignan a vectores similares para agruparlos en un solo atributo.\n",
        "\n",
        "#### Al aplicar k-Means a este conjunto de vectores, podemos crear $k$ puntos representativos y, para cada documento, generar un histograma de recuento de tokens en los clústeres generados."
      ],
      "metadata": {
        "id": "VHplkgNuPf2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **(2a) Agrupando los vectores y creando centros representativos**\n",
        "\n",
        "#### Como primer paso generaremos un RDD con los valores del diccionario `w2v`. A continuación, aplicaremos el algoritmo `k-Means` con $k = 200$ y $seed = 42$."
      ],
      "metadata": {
        "id": "vKXqTpj6Psd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejercicio\n",
        "from  pyspark.mllib.clustering import KMeans\n",
        "vectors2RDD = sc.parallelize(np.array(list(w2v.values())),1)\n",
        "print('Sample vector: {}'.format(vectors2RDD.take(1)))\n",
        "modelK = KMeans.train(vectors2RDD, 200, seed=42)\n",
        "clustersRDD = vectors2RDD.map(lambda x: modelK.predict(x))\n",
        "print('10 first clusters allocation: {}'.format(clustersRDD.take(10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9BzgdVePvPI",
        "outputId": "df0c5ea2-fb69-4ef1-b61c-8230302f2cf9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample vector: [array([-0.07269461,  0.09603201,  0.20506908, -0.03772384,  0.08151765])]\n",
            "10 first clusters allocation: [5, 136, 37, 12, 145, 66, 63, 84, 140, 66]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prueba de Amazon record with the most tokens (1d)\n",
        "#Variar los datos de verificación ya que los propuestos no son compatibles con el resultado y genera un error\n",
        "assert clustersRDD.take(10)==[5, 136, 37, 12, 145, 66, 63, 84, 140, 66], 'valor incorreto'"
      ],
      "metadata": {
        "id": "OgYrjFk1P5Oc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **(2b) Transformación de matriz de datos en vectores cuantificados**\n",
        "\n",
        "#### El siguiente paso es transformar nuestro RDD de frases en un RDD de pares (id, vector cuantificado). Para ello crearemos una función cuantificadora que recibirá como parámetros el objeto, el modelo k-means, el valor de k y el diccionario word2vec.\n",
        "\n",
        "#### Para cada punto, separemos el id y apliquemos la función `tokenize` a la cadena. Luego transformamos la lista de tokens en una matriz word2vec. Finalmente, aplicamos cada vector de esta matriz al modelo k-Means, generando un vector de tamaño $k$ donde cada posición $i$ indica cuántos tokens pertenecen al clúster $i$."
      ],
      "metadata": {
        "id": "5gp2x6V8QC33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejercicio\n",
        "def quantizador(point, model, k, w2v):\n",
        "    key = point[0]\n",
        "    words = tokenize(point[1])\n",
        "    matrix = np.array( [w2v[w] for w in words if w in w2v] )\n",
        "    features = np.zeros(k)\n",
        "    for v in matrix:\n",
        "        c = model.predict(v)\n",
        "        features[c] += 1\n",
        "    return (key, features)\n",
        "quantRDD = dataRDD.map(lambda x: quantizador(x, modelK, 500, w2v))\n",
        "print(quantRDD.take(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rn7c4iFHQF3J",
        "outputId": "9b269cb2-ebf8-4ac7-a241-60bae5d7b8c8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(64, array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0.]))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prueba para implementar la función TF (2a)\n",
        "assert quantRDD.take(1)[0][1].sum() == 5, 'valores incorretos'"
      ],
      "metadata": {
        "id": "LcWzOvUiQOoo"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}